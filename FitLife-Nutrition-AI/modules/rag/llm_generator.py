
import requests
import json
from typing import Optional
import sys

class LocalLLMGenerator:
    """Classe adaptative pour g√©n√©rer des r√©ponses avec LLM (Windows et VSCode)"""
    
    def __init__(self, model_name=None, base_url="http://localhost:11434", backend="ollama"):
        print("ü§ñ √âtape 6.1: Initialisation du LLM adaptatif...")
        
        self.model_name = model_name
        self.base_url = base_url
        self.backend = backend
        self.is_windows = sys.platform.startswith('win')
        
        print(f"   üíª Environnement d√©tect√©: {'Windows' if self.is_windows else 'Linux/Mac'}")
        
        # Initialiser selon le backend sp√©cifi√©
        if backend == "ollama":
            if self._try_ollama():
                self.backend = "ollama"
            else:
                print("   ‚ö†Ô∏è Ollama non disponible, passage en mode simple")
                self.backend = "simple"
        elif backend == "huggingface":
            if self._try_huggingface():
                self.backend = "huggingface"
            else:
                print("   ‚ö†Ô∏è HuggingFace non disponible, passage en mode simple")
                self.backend = "simple"
        else:
            print("   ‚ö†Ô∏è Mode simple activ√©")
            self.backend = "simple"
        
        # Templates de prompt
        self.prompt_templates = self._load_prompt_templates()
        
        print(f"   ‚úÖ LLM pr√™t (backend: {self.backend})")
    
    def _try_ollama(self):
        """Essaie de se connecter √† Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.model_name and self.model_name in model_names:
                    self.ollama_model = self.model_name
                elif model_names:
                    self.ollama_model = model_names[0]
                else:
                    print("   ‚ö†Ô∏è Ollama install√© mais pas de mod√®les")
                    return False
                
                print(f"   ‚úÖ Ollama connect√© avec mod√®le: {self.ollama_model}")
                return True
            
        except requests.exceptions.ConnectionError:
            print("   ‚ö†Ô∏è Ollama non d√©tect√© - Assurez-vous qu'Ollama est d√©marr√©")
            print("   üìå Commandes pour Windows:")
            print("       1. Ouvrir un terminal s√©par√©")
            print("       2. Ex√©cuter: ollama serve")
            print("       3. Revenir ici")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur Ollama: {e}")
        
        return False
    
    def _try_huggingface(self):
        """Essaie de charger un mod√®le HuggingFace"""
        try:
            from transformers import pipeline
            import torch
            
            hf_model = self.model_name or "google/flan-t5-small"
            print(f"   üì¶ Chargement du mod√®le HuggingFace: {hf_model}")
            
            if "t5" in hf_model.lower():
                self.hf_generator = pipeline(
                    "text2text-generation",
                    model=hf_model,
                    device=-1,
                    torch_dtype=torch.float32
                )
            else:
                self.hf_generator = pipeline(
                    "text-generation",
                    model=hf_model,
                    device=-1,
                    torch_dtype=torch.float32
                )
            
            print(f"   ‚úÖ Mod√®le HuggingFace charg√©")
            return True
            
        except Exception as e:
            print(f"   ‚ùå √âchec chargement HuggingFace: {str(e)[:80]}")
            return False
    
    def _load_prompt_templates(self):
        return {
            'nutrition_expert': """Tu es un nutritionniste expert avec 10 ans d'exp√©rience.

CONTEXTE NUTRITIONNEL:
{context}

QUESTION DU PATIENT:
{query}

INSTRUCTIONS POUR TA R√âPONSE:
1. Sois pr√©cis et scientifique
2. Utilise les donn√©es fournies
3. Donne des conseils pratiques
4. Mentionne les limites des donn√©es
5. Structure ta r√©ponse clairement

R√âPONSE DU NUTRITIONNISTE:""",

            'simple_assistant': """Tu es un assistant nutritionnel.

INFORMATIONS:
{context}

QUESTION:
{query}

R√©ponds de mani√®re utile et concise:""",

            'comparison_specialist': """Tu es un expert en comparaison nutritionnelle.

DONN√âES √Ä COMPARER:
{context}

DEMANDE DE COMPARAISON:
{query}

Fournis une analyse comparative d√©taill√©e:"""
        }
    
    def generate_response(self, query: str, context: str,
                         style: str = "nutrition_expert",
                         max_tokens: int = 500,
                         temperature: float = 0.7) -> str:
        print(f"üé® √âtape 6.2: G√©n√©ration de la r√©ponse (style: {style}, backend: {self.backend})...")
        
        template = self.prompt_templates.get(style, self.prompt_templates['simple_assistant'])
        prompt = template.format(context=context, query=query)
        
        if self.backend == "ollama":
            return self._generate_with_ollama(prompt, max_tokens, temperature)
        elif self.backend == "huggingface":
            return self._generate_with_huggingface(prompt, max_tokens, temperature)
        else:
            return self._fallback_response(query, context)
    
    def _generate_with_ollama(self, prompt: str, max_tokens: int, temperature: float) -> str:
        try:
            payload = {
                "model": self.ollama_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                full_response = result['response']
                cleaned_response = self._clean_response(full_response, prompt)
                
                print(f"   ‚úì R√©ponse g√©n√©r√©e ({len(cleaned_response.split())} mots)")
                return cleaned_response
            else:
                error_msg = f"Erreur Ollama: {response.status_code}"
                print(f"   ‚ùå {error_msg}")
                return self._fallback_response_from_prompt(prompt)
                
        except Exception as e:
            print(f"   ‚ùå Erreur Ollama: {e}")
            return self._fallback_response_from_prompt(prompt)
    
    def _generate_with_huggingface(self, prompt: str, max_tokens: int, temperature: float) -> str:
        try:
            generation_params = {
                "max_length": len(prompt.split()) + max_tokens,
                "temperature": temperature,
                "do_sample": True,
                "num_return_sequences": 1,
                "top_p": 0.9,
                "repetition_penalty": 1.1
            }
            
            result = self.hf_generator(prompt, **generation_params)
            
            if isinstance(result, list) and len(result) > 0:
                if 'generated_text' in result[0]:
                    full_response = result[0]['generated_text']
                else:
                    full_response = str(result[0])
            else:
                full_response = str(result)
            
            cleaned_response = self._clean_response(full_response, prompt)
            
            print(f"   ‚úì R√©ponse g√©n√©r√©e ({len(cleaned_response.split())} mots)")
            return cleaned_response
            
        except Exception as e:
            print(f"   ‚ùå Erreur HuggingFace: {e}")
            return self._fallback_response_from_prompt(prompt)
    
    def _clean_response(self, response: str, prompt: str) -> str:
        if prompt in response:
            response = response.split(prompt)[-1]
        
        stop_sequences = ["###", "Human:", "Assistant:", "\\n\\n\\n", "[INST]", "[/INST]"]
        for stop in stop_sequences:
            if stop in response:
                response = response.split(stop)[0]
        
        response = response.strip()
        
        if len(response.split()) > 400:
            sentences = response.split('. ')
            response = '. '.join(sentences[:8]) + '.'
        
        return response
    
    def _fallback_response_from_prompt(self, prompt: str) -> str:
        print("   ‚ö†Ô∏è Utilisation du mode fallback")
        
        lines = prompt.split('\\n')
        query = ""
        for line in lines:
            if "QUESTION:" in line or "Question:" in line:
                query = line.replace("QUESTION:", "").replace("Question:", "").strip()
                break
        
        return f"""Bas√© sur votre question "{query}", voici une analyse nutritionnelle:

Pour une r√©ponse plus d√©taill√©e, consultez les donn√©es nutritionnelles compl√®tes.

Note: Le syst√®me d'IA avanc√© est temporairement indisponible."""
    
    def _fallback_response(self, query: str, context: str) -> str:
        print("   ‚ö†Ô∏è Utilisation du mode fallback simple")
        
        lines = context.split('\\n')
        foods = []
        
        for line in lines:
            if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')) or 'Calories:' in line:
                foods.append(line)
        
        if foods:
            summary = "\\n".join(foods[:5])
            return f"""Bas√© sur votre question "{query}", voici ce que j'ai trouv√©:

{summary}

Pour une analyse plus d√©taill√©e, veuillez reformuler votre question."""
        else:
            return f"Je n'ai pas pu g√©n√©rer une r√©ponse d√©taill√©e pour votre question sur '{query}'. Voici les informations disponibles:\\n\\n{context[:500]}..."
